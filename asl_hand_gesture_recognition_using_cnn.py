# -*- coding: utf-8 -*-
"""ASL_Hand_Gesture_Recognition_Using_CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Hf8WWW0VPKz65AucPujBuIYeQTUpIPJ

# **Hand Gesture Recognition**

In this project I will walk through

1. Generate and preprocess my own data
2. Load and split data for training, validation and testing
3. Train a Convolutional Neural Network
4. Apply transfer learning to improve your model

# Part A. Data Collection


### American Sign Language

American Sign Language (ASL) is a complete, complex language that employs signs made by moving the
hands combined with facial expressions and postures of the body. It is the primary language of many
North Americans who are deaf and is one of several communication options used by people who are deaf or
hard-of-hearing.

The hand gestures representing English alphabet are shown below. I will focus on classifying a subset
of these hand gesture images using convolutional neural networks. Specifically, given an image of a hand
showing one of the letters A-I, we want to detect which letter is being represented.

![alt text](https://www.disabled-world.com/pics/1/asl-alphabet.jpg)


### Generating Data
I have produced my own images (three images each of Americal Sign Language gestures for letters A - I (total of 27 images))


### Cleaning Data
I made sure that
all the images are of the same size (224 x 224 pixels RGB), and have the hand in the center of the cropped
regions.

Building a CNN

Used PyTorch. nCode is vectorized, and does not contain obvious inefficiencies (for example, unecessary
for loops, or unnecessary calls to unsqueeze()).

## Part 1. Data Loading and Splitting

Split the data into training, validation, and test sets. Data splitting is not as trivial in this project. We want our test set to closely resemble the setting in which our model will be used. In particular, our test set should contain hands that are never seen in training!
"""

# load "Lab_3b_Gesture_Dataset" to Google Colab
from google.colab import files
uploaded = files.upload()

!unzip Lab_3b_Gesture_Dataset.zip

import os
import random
import matplotlib.pyplot as plt
from torchvision.datasets import ImageFolder
from torch.utils.data import random_split, DataLoader
import torchvision.transforms as transforms

# Resize images to 224x224, then transform all images  to PyTorch tensor
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

# Load the data into ImageFolder with the transformation
dataset = ImageFolder(root='Lab_3b_Gesture_Dataset', transform=transform)

# Extract all the unique participants
participants = set()
for img_path, _ in dataset.imgs:
    participant_num = img_path.split('/')[-1].split('_')[0]  # Extracts participant number from filename
    participants.add(participant_num)

# Split participants into training, validation, and test
participants = list(participants)
random.shuffle(participants)
num_participants = len(participants)
train_participants = participants[:int(0.7 * num_participants)]
val_participants = participants[int(0.7 * num_participants):int(0.85 * num_participants)]
test_participants = participants[int(0.85 * num_participants):]

# Split images based on participants
train_imgs = [img for img, _ in dataset.imgs if img.split('/')[-1].split('_')[0] in train_participants]
val_imgs = [img for img, _ in dataset.imgs if img.split('/')[-1].split('_')[0] in val_participants]
test_imgs = [img for img, _ in dataset.imgs if img.split('/')[-1].split('_')[0] in test_participants]

print(f"Total Number of Unique Participants: {num_participants}")
print(f"Number of training images: {len(train_imgs)}")
print(f"Number of validation images: {len(val_imgs)}")
print(f"Number of test images: {len(test_imgs)}")

"""Although we seem to have a large dataset, because we only have 91 unique participants, we need to be considerate with the use of the data. For this reason I will split the data set as follows:

Training set: 70%
Validation set: 15%
Test set: 15%

To ensure the accuracy of the model, we must ensure that the test set does not have any hands that were seen before in the training set, or the validation set. In other words, each unique person's hands should only appear in one of the sets (i.e. training, validation, or testing). This is because, when using this model in real life, we should be able to feed never before seen hands and still get an accurate prediction. By ensuring all sets are mutually exclusive, we are ensuring that in the training phase, the model does not have the opportunity to "memorize" a hand. Further, it ensures that the testing set will provide an accurate picture of how the model would work in real life.

## Part 2. Model Building and Sanity Checking

### Part (i) Convolutional Network

Built a convolutional neural network model that takes the (224x224 RGB) image as input, and predicts the gesture
letter.
"""

import torch.nn as nn

class ASLNet(nn.Module):
    def __init__(self, num_classes=9): # 9 since A-I is 9 letters
        super(ASLNet, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32,64, 3, 1, 1)

        # Pooling layer
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)

        # Fully Connected layers
        self.fc1 = nn.Linear(64 * 56 * 56, 512)  # after 2 max pooling, the size becomes 224/2/2 = 56
        self.fc2 = nn.Linear(512, num_classes)

        # Activation function
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))

        # Flatten the tensor
        x = x.view(x.size(0), -1)

        x = self.relu(self.fc1(x))

        x = self.fc2(x)
        return x

model = ASLNet()
print(model)

"""**Convolution Layers**

>**self.conv1**

>>*Input Channels (3)*: Corresponds to the RGB channels of the input.
*Output Channels (32)*: Network starts w/ a relatively small number of filters because in initial layers, want the network to learn basic features like edges, corners, etc.
*Kernel Size (3):* 3x3 kernel allows network to capture local patterns and is computationally efficient.                    
*Padding (1):* For spatial dimensions to remain the same before and after this layer due to the 3x3 kernel size.

>**self.conv2**

>>**Input Channels (32)**: Matches the output channels of conv1.            
**Output Channels (64)**: Double number of filters to allow network to capture more complex patterns.   
**Kernel Size, Padding, and Stride:** Same reasoning as conv1.

**Pooling Layer (self.pool):**

>**Max Pooling:** Max pooling used to reduce the spatial dimensions of the image while retaining the most important information (i.e., the maximum value in a local region) - helps to reduce computational load and makes network more translation invariant.          
**Kernel Size (2) and Stride (2**): Halve the spatial dimensions with each pooling operation.

**Fully Connected Layers:**
>**self.fc1:** After two max-pooling operations on a 224x224 image, size becomes 56x56. With 64 channels from conv2, that's a total of 64x56x56 nodes feeding into this layer. We connect these to 512 nodes in this layer.    
**self.fc2:** This layer connects the 512 nodes from fc1 to the 9 output nodes, corresponding to the 9 gesture letters (A-I). This is the output layer of the network.

**Activation Function (self.relu):**
>**ReLU:** The Rectified Linear Unit (ReLU) is computationally efficient and helps with the vanishing gradient problem. Also introduces non-linearity into the model.

**Dropout (self.dropout):**
>**Dropout Rate (0.5):** Included dropout as a regularization technique. Dropout will randomly setting some nodes to zero during training, this can help prevent overfitting.

### Part (ii) Training Code
"""

def get_accuracy(model, data_loader, device):
    """ Compute the accuracy of the model on a given dataset """
    correct = 0
    total = 0
    for imgs, labels in data_loader:
        imgs, labels = imgs.to(device), labels.to(device)

        output = model(imgs)
        # select index with maximum prediction score
        pred = output.max(1, keepdim=True)[1]
        correct += pred.eq(labels.view_as(pred)).sum().item()
        total += imgs.shape[0]
    return correct / total

import torch
import torch.optim as optim

# Check for CUDA availability and set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Training on {device}")

# Move the model to the selected device
model = ASLNet().to(device)

# Training function
def train(model, train_data, val_data, batch_size=64, num_epochs=1, learning_rate=0.001, save_path="checkpoint.pth"):

    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_data, batch_size=batch_size)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    iters, losses, train_acc, val_acc = [], [], [], []
    n = 0 # the number of iterations

    for epoch in range(num_epochs):
        for imgs, labels in iter(train_loader):
            imgs, labels = imgs.to(device), labels.to(device)

            out = model(imgs)
            loss = criterion(out, labels)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            # save the current training information
            iters.append(n)
            losses.append(float(loss)/batch_size)
            train_acc.append(get_accuracy(model, train_loader, device))
            val_acc.append(get_accuracy(model, val_loader, device))
            n += 1

        # Save model checkpoint after every epoch
        torch.save(model.state_dict(), save_path)

    # Plotting training history
    plt.title("Training Curve")
    plt.plot(iters, losses, label="Train")
    plt.xlabel("Iterations")
    plt.ylabel("Loss")
    plt.show()

    plt.title("Training Curve")
    plt.plot(iters, train_acc, label="Train")
    plt.plot(iters, val_acc, label="Validation")
    plt.xlabel("Iterations")
    plt.ylabel("Training Accuracy")
    plt.legend(loc='best')
    plt.show()

    print("Final Training Accuracy: {}".format(train_acc[-1]))
    print("Final Validation Accuracy: {}".format(val_acc[-1]))

# Split the dataset based on participants' images
train_data = [dataset[i] for i, (img, _) in enumerate(dataset.imgs) if img in train_imgs]
val_data = [dataset[i] for i, (img, _) in enumerate(dataset.imgs) if img in val_imgs]

train(model, train_data, val_data, num_epochs=10, learning_rate=0.001)

"""1. Loss Function: CrossEntropyLoss
The problem is a multi-class classification, where each image should belong to one of the possible ASL gestures. CrossEntropyLoss is designed for this type of task. CrossEntropyLoss also combines a Softmax activation function and a negative log likelihood loss. It ensures that the probabilities predicted by the model for all classes sum to 1, and the loss is high when the prediction for the correct class is low. Finally it's more numerically stable than calculating softmax and then the log likelihood as two separate steps.

2. Optimizer: Adam
Adam adjusts the learning rate for each parameter during training, which can lead to faster convergence and reduced need for a finely-tuned global learning rate. It also incorporates the benefits of both AdaGrad and RMSProp algorithms, maintaining a moving average of past gradients. This can help overcome saddle points and local minima in the loss landscape. Adam has hyperparameters like beta1 and beta2 that are generally set to default values of 0.9 and 0.999 respectively, which work well in most cases, making it easier to use without extensive hyperparameter tuning.

### Part (iii) “Overfit” to a Small Dataset
One way to sanity check our neural network model and training code is to check whether the model is capable
of “overfitting” or “memorizing” a small dataset. A properly constructed CNN with correct training code
should be able to memorize the answers to a small number of images quickly.

I construct a small dataset (e.g. just the images that you have collected), then show that then model and
training code is capable of memorizing the labels of this small data set.

With a large batch size (e.g. the entire small dataset) and learning rate that is not too high, I should be
able to obtain a 100% training accuracy on that small dataset relatively quickly (within 200 iterations).
"""

# UPloading personal small dataset
from google.colab import files
uploaded = files.upload()

!unzip ASL_Resized.zip

from torchvision.datasets import ImageFolder
transform_ = transforms.Compose([
    transforms.ToTensor()
])

small_dataset = ImageFolder(root='ASL_Resized', transform=transform_)

model = ASLNet().to(device)
# Train the model on the small dataset
# Batch_size equal to len of the small dataset to use whole dataset in 1 batch
train(model, small_dataset, small_dataset, batch_size=len(small_dataset), num_epochs=150, learning_rate=0.001)
use_cuda = torch.cuda.is_available()

"""## Part 3. Hyperparameter Search
### Part (i)

In part ii), when the training code was initially run, we observed signs of overfitting i.e. high training accuracy vs lower validation accuracy (96% vs. 67% respectively). As such, we should focus on the hyperparameters that directly impact regularization. For this reason, I believe that the drop out rate and learning rate are important to tune. ALtering the number of filters in the convolution layers can also help with overfitting and is directly relate dto the mosel's architechture.

> **Learning Rate:** The learning rate determines the step size at each iteration while moving toward a minimum of the loss function. If it's too large, the model might overshoot the optimal point. If it's too small, training might be very slow or get stuck in a local minimum. Since we observed good training accuracy but lower validation accuracy, we can try reducing the learning rate slightly. ALthough this might slow down the convergence, it might lead to a more generalized model.

> **Batch Size**: Since the is showing signs of overfitting, using a smaller batch size might introduce noise into the gradient, providing implicit regularization and could potentially reducing overfitting. We are currently using a batch size of 64, we will try to reduce it to 32.

>**Number of Convolutional Filters (in layers conv1 and conv2)**: Determines the capacity of the network to extract features from input images. Can increase and/or decrease these numbers to see if we can get a better balance between model capacity and overfitting.

### Part (ii)

Tune the hyperparameters
"""

model = ASLNet().to(device)
train_two = train(model, train_data, val_data, num_epochs=10, learning_rate=0.0005, save_path="train_two.pth")

train_three = train(model, train_data, val_data, batch_size=32, num_epochs=10, learning_rate=0.001)

"""**Dropout Rate:** Can help in preventing overfitting by improving the model's generalization to the validation set. We will experiment by using a dropout rate of 0.5 to see the impact on performance."""

import torch.nn as nn

class ASLNet_drop(nn.Module):
    def __init__(self, num_classes=9): # 9 since A-I is 9 letters
        super(ASLNet_, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32,64, 3, 1, 1)

        # Pooling layer
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)

        # Fully Connected layers
        # self.fc1 = nn.Linear(64 * 56 * 56, 512) <- OG
        self.fc1 = nn.Linear(128 * 56 * 56, 512) # <- New
        self.fc2 = nn.Linear(512, num_classes)

        # Activation function
        self.relu = nn.ReLU()

        # Dropout for regularization
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))

        # Flatten the tensor
        x = x.view(x.size(0), -1)

        x = self.dropout(self.relu(self.fc1(x))) # Included drop out when training normal batch size

        x = self.fc2(x)
        return x

model = ASLNet_drop()
print(model)

model = ASLNet_drop().to(device)
train_four = train(model, train_data, val_data, num_epochs=10, learning_rate=0.001)

train_five = train(model, train_data, val_data, batch_size=64, num_epochs=10, learning_rate=0.001)
#changed the dropout to 0.6 in the model

"""Adjust the number of convolution filters:"""

import torch.nn as nn

class ASLNet_(nn.Module):
    def __init__(self, num_classes=9): # 9 since A-I is 9 letters
        super(ASLNet_, self).__init__()

        # Convolutional layers
        # self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1) <- og
        # self.conv2 = nn.Conv2d(32,64, 3, 1, 1) <- og

        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1) # <- NEW
        self.conv2 = nn.Conv2d(64, 128, 3, 1, 1) # <- NEW

        # Pooling layer
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)

        # Fully Connected layers
        # self.fc1 = nn.Linear(64 * 56 * 56, 512) <- OG
        self.fc1 = nn.Linear(128 * 56 * 56, 512) # <- New
        self.fc2 = nn.Linear(512, num_classes)

        # Activation function
        self.relu = nn.ReLU()

        # Dropout for regularization
        self.dropout = nn.Dropout(0.6) # trained with 0.5 until tuning dropout - then used 0.6

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))

        # Flatten the tensor
        x = x.view(x.size(0), -1)

        x = self.dropout(self.relu(self.fc1(x))) # Included drop out when training normal batch size
        # x = self.relu(self.fc1(x)) # commented out dropout when training small_batch

        x = self.fc2(x)
        return x

model = ASLNet_()
print(model)

model = ASLNet_().to(device)
train_four = train(model, train_data, val_data, batch_size=64, num_epochs=10, learning_rate=0.001)

"""### Part (iii)

The best model out of all the ones that were trained was model_two because it showed the greatest increase in the final validation accuracy and did not show a significant decrease in the final training accuracy.

For model_two:
>Final Training Accuracy: 0.9567279193835211
>Final Validation Accuracy: 0.7706666666666667

Although this model did show some evidence of overfitting, since the gap between the training accuracy and the validation accuracy was the smallest of all of the training sessions and the final validation accuracy was relatively high and continued to increase (although slope was smaller in later epochs), this was the best model.

### Part (iv)
"""

# Make list of test data samples based on test images
test_data = [dataset[i] for i, (img, _) in enumerate(dataset.imgs) if img in test_imgs]
# DataLoader for test set
test_loader = DataLoader(test_data, batch_size=64)

model = ASLNet().to(device)
# Load the model state
model.load_state_dict(torch.load("train_two.pth" ))
# Set model to evaluation mode
model.eval()

test_accuracy = get_accuracy(model, test_loader, device)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

"""## Part 4. Transfer Learning
For many image classification tasks, it is generally not a good idea to train a very large deep neural network
model from scratch due to the enormous compute requirements and lack of sufficient amounts of training
data.

One of the better options is to try using an existing model that performs a similar task to the one you need
to solve. This method of utilizing a pre-trained network for other similar tasks is broadly termed **Transfer
Learning**. I will use Transfer Learning to extract features from the hand gesture
images. Then, I will train a smaller network to use these features as input and classify the hand gestures.

"""



"""### Part (i)
Here is the code to load the AlexNet network, with pretrained weights. When you first run the code, PyTorch
will download the pretrained weights from the internet.
"""

import torchvision.models
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
alexnet = torchvision.models.alexnet(pretrained=True).to(device)

"""The alexnet model is split up into two components: *alexnet.features* and *alexnet.classifier*. The
first neural network component, *alexnet.features*, is used to compute convolutional features, which are
taken as input in *alexnet.classifier*.

The neural network alexnet.features expects an image tensor of shape Nx3x224x224 as input and it will
output a tensor of shape Nx256x6x6 . (N = batch size).

Compute the AlexNet features for each training, validation, and test data. Here is an example code
snippet showing how you can compute the AlexNet features for some images.
"""

# img = ... a PyTorch tensor with shape [N,3,224,224] containing hand images ...
# features = alexnet.features(img)

"""**Save the computed features**."""

def extract_features(dataloader):
    all_features = []
    all_labels = []

    with torch.no_grad():
        for imgs, labels in dataloader:
            imgs = imgs.to(device)

            features = alexnet.features(imgs)
            all_features.append(features.to(device))
            all_labels.append(labels)

    return torch.cat(all_features, 0), torch.cat(all_labels, 0)

train_loader = DataLoader(train_data, batch_size=64, shuffle=False)
val_loader = DataLoader(val_data, batch_size=64, shuffle=False)

train_features, train_labels = extract_features(train_loader)
val_features, val_labels = extract_features(val_loader)

torch.save((train_features, train_labels), 'train_features.pt')
torch.save((val_features, val_labels), 'val_features.pt')

"""### Part (ii)
Build a convolutional neural network model that takes as input these AlexNet features, and makes a
prediction.
"""

class FeatureClassifier(nn.Module):
    def __init__(self, num_classes=9):  # 9 for letters A-I
        super(FeatureClassifier, self).__init__()

        # since output of alexnet.features is Nx256x6x6,
        #input for the FC layer would be 256*6*6
        self.fc1 = nn.Linear(256*6*6, 512)
        self.fc2 = nn.Linear(512, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        # Flatten the tensor
        x = x.view(x.size(0), -1)
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Instantiate the model
model = FeatureClassifier().to(device)

"""(self.fc1 and self.fc2): Since we're using the features extracted by AlexNet, the new network consists mainly of fully connected layers. The first fully connected layer (512 hidden units) is responsible for reducing dimensions of the input and extracting more dense and relevant features. Second fully connected layer reduces the dimension to the number of classes (i.e., 9) and gives the logits for each class.

(self.relu): ReLU is used as an activation function to introduce non-linearity into the model. It replaces all negative values in the tensor with zeros.

### Part (iii)
Train new network, including hyperparameter tuning.
"""

# tensor = torch.from_numpy(tensor.detach().numpy())

# Constructing datasets with extracted features and corresponding labels
train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)
val_dataset = torch.utils.data.TensorDataset(val_features, val_labels)

# Create DataLoaders
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)

# Adjust get_accuracy function to handle features
def get_accuracy_features(model, data_loader, device):
    model.eval()  # model set to evaluation mode
    correct = 0
    total = 0
    with torch.no_grad():
        for features, labels in data_loader:
            features, labels = features.to(device), labels.to(device)

            output = model(features)
            pred = output.max(1, keepdim=True)[1]
            correct += pred.eq(labels.view_as(pred)).sum().item()
            total += features.shape[0]
    model.train()  # model back to training mode
    return correct / total

# Adjust training function to use get_accuracy_features
def train_features_model(model, train_data, val_data, batch_size=64, num_epochs=1, learning_rate=0.001, save_path="features_checkpoint.pth"):
    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_data, batch_size=batch_size)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    iters, losses, train_acc, val_acc = [], [], [], []
    n = 0 # number of iterations

    for epoch in range(num_epochs):
        for imgs, labels in iter(train_loader):
            imgs, labels = imgs.to(device), labels.to(device)
            model.train()
            out = model(imgs)
            loss = criterion(out, labels)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            # save current training info
            iters.append(n)
            losses.append(float(loss))
            train_acc.append(get_accuracy_features(model, train_loader, device))
            val_acc.append(get_accuracy_features(model, val_loader, device))
            n += 1

        # Save model checkpoint after each epoch
        torch.save(model.state_dict(), save_path)

    plt.title("Training Curve")
    plt.plot(iters, losses, label="Train")
    plt.xlabel("Iterations")
    plt.ylabel("Loss")
    plt.show()

    plt.title("Training Curve")
    plt.plot(iters, train_acc, label="Train")
    plt.plot(iters, val_acc, label="Validation")
    plt.xlabel("Iterations")
    plt.ylabel("Accuracy")
    plt.legend(loc='best')
    plt.show()

    print("Final Training Accuracy: {}".format(train_acc[-1]))
    print("Final Validation Accuracy: {}".format(val_acc[-1]))

train_features_model(model, train_dataset, val_dataset, save_path="_transfer_learning.pth")

"""### Part (iv)

"""

#Extract Features from Test Set
test_loader = DataLoader(test_data, batch_size=64, shuffle=False)
test_features, test_labels = extract_features(test_loader)
torch.save((test_features, test_labels), 'test_features.pt')

#Load the trained model
model = FeatureClassifier().to(device)
model.load_state_dict(torch.load("_transfer_learning.pth"))

# Evaluate the model on test set
test_dataset = torch.utils.data.TensorDataset(test_features, test_labels)
test_loader = DataLoader(test_dataset, batch_size=64)
test_accuracy = get_accuracy_features(model, test_loader, device)
print(f"Test Accuracy (Transfer Learning): {test_accuracy * 100:.2f}%")

"""AlexNet is a deep neural network model that was trained on the ImageNet dataset, which contains over a million images from 1000 different classes. By using the features learned by AlexNet, the model was able to capitalize on the knowledge AlexNet gained from seeing a wide variety of images, which generally leads to better feature representations for many visual tasks.IN comparison, the custom CNN from before starteD its learning process from scratch. Without any prior knowledge, it only learned from the data that was provided to it.

AlexNet also has a deeper architecture and more parameters than our custom CNN. SInce deeper networks tend to capture hierarchical and intricate patterns in data, they can be more advantageous for classification tasks.

By using AlexNet for feature extraction and training only the classifier layers on top of it, we decoupled feature extraction from classification. This can sometimes yield better results, especially if the pre-trained model's features are robust and generalizable.

Transfer learning is especially powerful when we have a relatively small dataset. Since the gesture dataset was small, the custom CNN might not have had enough data to learn robust features from scratch. THis is why borrowing features from a model trained on a much larger dataset can be highly beneficial.

AlexNet is a deeper network which comes with more parameters and can be more likely to overfit on a small datasets. However, by using only the features from AlexNet and training a separate, simpler classifier on top, we reduced the risk of overfitting. In contrast, the CNN from before might have overfit more easily since there may not have been enough regularizing techniques in place.

## Part 5. Testing on New Data

### Part (i)
"""

from google.colab import files
uploaded = files.upload()

!unzip ASL_Resized.zip

from torchvision.datasets import ImageFolder
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

small_dataset = ImageFolder(root='ASL_Resized', transform=transform_)

"""### Part (ii)

"""

test_dataset = torch.utils.data.TensorDataset(test_features, test_labels)
test_loader = DataLoader(test_dataset, batch_size=64)
test_accuracy = get_accuracy_features(model, test_loader, device)
print(f"Test Accuracy (Transfer Learning): {test_accuracy * 100:.2f}%")

small_loader = DataLoader(small_dataset, batch_size=64, shuffle=False)
model_path = "_transfer_learning.pth"
model = FeatureClassifier().to(device)
model.load_state_dict(torch.load(model_path))
model.eval()
small_test_accuracy = get_accuracy_features(model, small_loader, device)
print(f"Accuracy on small_dataset: {small_test_accuracy * 100:.2f}%")

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])
_small_dataset = ImageFolder(root='ASL_Resized', transform=transform)

small_loader = DataLoader(_small_dataset, batch_size=64, shuffle=False)
model_path = "train_two.pth"
model = ASLNet().to(device)
model.load_state_dict(torch.load(model_path))
model.eval()
small_test_accuracy_ = get_accuracy(model, small_loader, device)
print(f"Accuracy on small_dataset: {small_test_accuracy_ * 100:.2f}%")

import pandas as pd
results = {
    'Model Type': ['CNN with Transfer Learning (Big Dataset)', 'CNN with Transfer Learning (Small Dataset)', 'CNN Best Model from Part 3 (Big Dataset)', 'CNN Best Model from Part 3 (Small Dataset)'],
    'Test Accuracy': [test_accuracy * 100, small_test_accuracy * 100, '71.27', small_test_accuracy_ * 100]
}

df = pd.DataFrame(results)
df

"""### Part (iii)

**CNN with Transfer Learning**
>Final Training Accuracy: 99.46%

>Final Validation Accuracy: 87.2%

>Test (with og dataset):  85.48%

>Test (with personal dataset):  100%

Transfer learning allows a pre-trained model (typically trained on a much larger dataset) to be fine-tuned on a specific task. Given that it achieved 85.48% on the original dataset suggests that the model benefited from the generic features learned during pre-training, leading to decent performance when fine-tuned.

A 100% accuracy might be suspicious. This might indicate overfitting, where the model has learned the personal dataset rather than generalizing. Overfitting is more likely when the dataset is small because there's not enough diversity in the data to prevent the model from simply memorizing it.

Overall with the transfer learning the final training, validation and test accuracies were higher than without transfer learning.

**CNN Best Model from Part 3**
> Final Training Accuracy: 99.35%

> Final Validation Accuracy: 70.40%

> Test (W/ OG Dataset) - 71.27%

> Test (w/ Personal Dataset) -85.18%

This model achieved a 71.27% accuracy on the original dataset. The lower performance compared to the transfer-learned model suggests the power of transfer learning; pre-trained models can leverage a lot of previously learned information that models trained from scratch lack.

In the real-world, performance would likely vary. This is because real-world data can have much more variability than controlled datasets. Lighting conditions, hand positions, occlusions, skin colors, and backgrounds can all affect performance.

TO improve the results, we can increase the diversity of the training data without collecting new data. Techniques might include random rotations, zooms, shifts, flips, and color perturbations. We can also use techniques like dropout or weight decay to reduce overfitting, especially for models showing suspiciously high accuracies. Combine predictions from multiple models to improve overall performance as well and reduce the likelihood of specific model biases.

We can systematically adjust hyperparameters using methods like grid search or random search. More data generally helps improve the model's generalization capability so we could benifit from training on a larger dataset.
"""